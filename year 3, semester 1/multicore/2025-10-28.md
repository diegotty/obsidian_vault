---
created: 2025-03-02T17:41
updated: 2025-10-28T17:45
---
## amdahl’s law
strong scaling
each program has some part of it which *cannot* be parallelized (e.g. I/O operations, sending/receiving data over the network, …). we calculate this fraction as the *serial fracttion* $1-\alpha$
- so a fraction $0 \leq \alpha\leq1$ can be parallelized

*amdahl’s law* says that the speedup is limited by the serial fraction
>[!info] img
![[Pasted image 20251028171505.png]]
we can divide $p$ by $n$ (by parallelizing it), however we can’t speedup $s$

speedup: tempo seriale / tempo parallelo
so we can’t obtain the ideal speedup we talked about earlier

$$
T_{\text{parallel}}(p) = (1-\alpha)T_{\text{serial}}+\alpha \frac{T_{\text{serial}}}{p}
$$
we then use amdahl’s law to calculate the speedup: 
$$
\frac{T_{\text{serial}}}{T_{\text{parallel}}(p) = (1-\alpha)T_{\text{serial}}+\alpha \frac{T_{\text{serial}}}{p}}
$$


calculates the upper bound of the speedup (with any amount of processors)


how do we calculate the percentage ?

>[!info] chart
![[Pasted image 20251028172537.png]]

## gustafson’s law
weak scaling
if we consider weak scaling, the parallel fraction increases with the problem size
carico di lavoro per ogni processo resta costante

in strong scaling, se l’input è piccolo, si crea tanto overhead (costa di più gestire che effettivamente calcolare x ogni processo)

collective functions get worse in performance the more processes are inside the communicator

CPU lingo
x core e y thread
context switching is very expensive !
having 2 threads in a core means that, with the same ALU, there are resources to save 2 process contexts. that way, context switching is faster
## example: sum between vectors

serial code:
```c
	void vector_sum(double x[], double y[], double z[], int n){
		int i;
		for(i = 0; i < n i++)
			z[i] = x[i] + y[i];
	}
```

`MPI_Scatter`
it can be used in a function that reads in an entire vector on process 0, and sends the needed component to each of the processes
- `int MPI_Scatter(void* send_buf_p, int send_count, MPI_Datatype send_type, void* recv_buf_p, int recv_count, MPI_Datatype recv_type, int src_proc, MPI_Comm comm)`


when the number of elements is not evenly fractionable by the number of processes, we need to use a different version of the scatter: `MPI_Scatterv`