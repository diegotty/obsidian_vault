---
created: 2025-03-02T17:41
updated: 2025-10-28T17:33
---
## amdahl’s law
strong scaling
each program has some part of it which *cannot* be parallelized (e.g. I/O operations, sending/receiving data over the network, …). we calculate this fraction as the *serial fracttion* $1-\alpha$
- so a fraction $0 \leq \alpha\leq1$ can be parallelized

*amdahl’s law* says that the speedup is limited by the serial fraction
>[!info] img
![[Pasted image 20251028171505.png]]
we can divide $p$ by $n$ (by parallelizing it), however we can’t speedup $s$

speedup: tempo seriale / tempo parallelo
so we can’t obtain the ideal speedup we talked about earlier

$$
T_{\text{parallel}}(p) = (1-\alpha)T_{\text{serial}}+\alpha \frac{T_{\text{serial}}}{p}
$$
we then use amdahl’s law to calculate the speedup: 
$$
\frac{T_{\text{serial}}}{T_{\text{parallel}}(p) = (1-\alpha)T_{\text{serial}}+\alpha \frac{T_{\text{serial}}}{p}}
$$


calculates the upper bound of the speedup (with any amount of processors)


how do we calculate the percentage ?

>[!info] chart
![[Pasted image 20251028172537.png]]

## gustafson’s law
weak scaling
if we consider weak scaling, the parallel fraction increases with the problem size
carico di lavoro per ogni processo resta costante

in strong scaling, se l’input è piccolo, si crea tanto overhead (costa di più gestire che effettivamente calcolare x ogni processo)

collective functions get worse in performance the more processes are inside the communicator

CPU lingo
x core e y thread
context switching is very expensive !
having 2 threads in a core means that, with the same ALU, there are resources to save 2 process contexts. that way, context switching is faster