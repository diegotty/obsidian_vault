---
created: 2025-10-29T21:51
updated: 2025-11-03T14:08
---
## amdahl’s law
each program has some part of it which *cannot* be parallelized (e.g. I/O operations, sending/receiving data over the network, …). we calculate this fraction as the *serial fraction*: $1-\alpha$
- with $0 \leq \alpha\leq1$ as the fraction of the program that can be parallelized

*amdahl’s law* says that the speedup is limited by the serial fraction
>[!info] representation
![[Pasted image 20251028171505.png]]
we can divide $p$ by $n$ (by parallelizing it), however we can’t speedup $s$

so we need to re-calculate our $T_{parallel}(p)$
$$
T_{\text{parallel}}(p) = (1-\alpha)T_{\text{serial}}+\alpha \frac{T_{\text{serial}}}{p}
$$
we then use amdahl’s law to calculate the speedup: 
$$
S(p) = \frac{T_{\text{parallel}}(p)} {(1-\alpha)T_{\text{serial}}+\alpha \frac{T_{\text{serial}}}{p}}
$$


calculates the upper bound of the speedup (with any amount of processors)


how do we calculate the percentage ?

>[!info] chart
![[Pasted image 20251028172537.png]]

## gustafson’s law
weak scaling
if we consider weak scaling, the parallel fraction increases with the problem size
carico di lavoro per ogni processo resta costante

in strong scaling, se l’input è piccolo, si crea tanto overhead (costa di più gestire che effettivamente calcolare x ogni processo)

collective functions get worse in performance the more processes are inside the communicator

CPU lingo
x core e y thread
context switching is very expensive !
having 2 threads in a core means that, with the same ALU, there are resources to save 2 process contexts. that way, context switching is faster
## example: sum between vectors

serial code:
```c
	void vector_sum(double x[], double y[], double z[], int n){
		int i;
		for(i = 0; i < n i++)
			z[i] = x[i] + y[i];
	}
```

`MPI_Scatter`
it can be used in a function that reads in an entire vector on process 0, and sends the needed component to each of the processes
- `int MPI_Scatter(void* send_buf_p, int send_count, MPI_Datatype send_type, void* recv_buf_p, int recv_count, MPI_Datatype recv_type, int src_proc, MPI_Comm comm)`


when the number of elements is not evenly fractionable by the number of processes, we need to use a different version of the scatter: `MPI_Scatterv`


`MPI_IN_PLACE` optimizes (instead of requiring a new buffer for process 0, it uses the fact that process 0 already has the entire buffer)


gather gathers following the ranks as the order


matrices get stored as rows in memory

linearize matrices ! we have to access it in a different way but its fine


how to parallelize matrix multiplication
- we can give matrix and array to every process
- we can divide the rows (and give them to diff processes ) and give (broadcast) the array to each process. then we gather


let’s be annoying. we use the output of the multiplication as the new array to multiply A by


if while writing code i use 2 collective functions in a row (es: reduce + broadcast) , there probably is another collective that does exactly that but in one (es: allreduce)


allgather has no root. everyone sends and receives something


diff machines could implement types with different sizes