---
created: 2025-10-29T21:51
updated: 2025-11-03T14:38
---
## amdahl’s law
we focus on *weak scaling*, where the problem size does not increase:
each program has some part of it which *cannot* be parallelized (e.g. I/O operations, sending/receiving data over the network, …). we calculate this fraction as the *serial fraction*: $1-\alpha$
- with $0 \leq \alpha\leq1$ as the fraction of the program that can be parallelized

*amdahl’s law* says that the speedup is limited by the serial fraction
>[!info] representation
![[Pasted image 20251028171505.png]]
we can divide $p$ by $n$ (by parallelizing it), however we can’t speedup $s$

so we need to re-consider how we think of $T_{parallel}(p)$:
$$
T_{\text{parallel}}(p) = (1-\alpha)T_{\text{serial}}+\alpha \frac{T_{\text{serial}}}{p}
$$
we also need to re-consider the speedup $S(p)$:
$$
S(p) = \frac{T_{\text{serial}}} {(1-\alpha)T_{\text{serial}}+\alpha \frac{T_{\text{serial}}}{p}}
$$

this puts an *upper bound* of the speedup *for any amount of processors*, as
$$
\lim_{ p \to \infty } S(p) = \frac{1}{1-\alpha}
$$
>[!example] example
if $\alpha=0.6$, it means we can expect a speedup of, at most, 2.5
if $\alpha=0.8$, it means we can expect a speedup of, at most, 5
>
>to be able to scale up to 10000, processes, we need to have a $\alpha \geq 0.9999$


>[!info] chart
![[Pasted image 20251028172537.png]]
this chart shows the relationship between $alpha$ and the possible scalability

however, in pratice this could get worse than the hypothesized best case scenario, as with small problem sizes, using more processes creates a lot of overhead 
>[!info] law of diminishing returns in full effect
![[Pasted image 20251103143640.png]]
## gustafson’s law
if we consider *weak scaling*, the parallel fraction increases with the problem size ! also, because the increase in problem size is matched with an increase of processes, the work assigned to each processor remains constant
- the serial time remains constant, because the serial time usually deals with the overall setup and tear-down of the program, not the per-processor work.
- the parallel time increases, as more processes implies more communication and synchronization between them, which increases the overhead. in fact, collective functions get worse in performance the more processes there are inside the communicator
this is also known as *scaled speedup*:
$$
S(n,p) = (1-\alpha) + \alpha p
$$

>[!info] representation
![[Pasted image 20251103143125.png]]

## example: sum between vectors
serial code:
```c
	void vector_sum(double x[], double y[], double z[], int n){
		int i;
		for(i = 0; i < n i++)
			z[i] = x[i] + y[i];
	}
```

`MPI_Scatter`
it can be used in a function that reads in an entire vector on process 0, and sends the needed component to each of the processes
- `int MPI_Scatter(void* send_buf_p, int send_count, MPI_Datatype send_type, void* recv_buf_p, int recv_count, MPI_Datatype recv_type, int src_proc, MPI_Comm comm)`


when the number of elements is not evenly fractionable by the number of processes, we need to use a different version of the scatter: `MPI_Scatterv`


`MPI_IN_PLACE` optimizes (instead of requiring a new buffer for process 0, it uses the fact that process 0 already has the entire buffer)


gather gathers following the ranks as the order


matrices get stored as rows in memory

linearize matrices ! we have to access it in a different way but its fine


how to parallelize matrix multiplication
- we can give matrix and array to every process
- we can divide the rows (and give them to diff processes ) and give (broadcast) the array to each process. then we gather


let’s be annoying. we use the output of the multiplication as the new array to multiply A by


if while writing code i use 2 collective functions in a row (es: reduce + broadcast) , there probably is another collective that does exactly that but in one (es: allreduce)


allgather has no root. everyone sends and receives something


diff machines could implement types with different sizes

CPU lingo
x core e y thread
context switching is very expensive !
having 2 threads in a core means that, with the same ALU, there are resources to save 2 process contexts. that way, context switching is faster