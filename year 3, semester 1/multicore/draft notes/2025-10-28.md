---
created: 2025-10-29T21:51
updated: 2025-11-03T14:50
---
## amdahl’s law
we focus on *weak scaling*, where the problem size does not increase:
each program has some part of it which *cannot* be parallelized (e.g. I/O operations, sending/receiving data over the network, …). we calculate this fraction as the *serial fraction*: $1-\alpha$
- with $0 \leq \alpha\leq1$ as the fraction of the program that can be parallelized

*amdahl’s law* says that the speedup is limited by the serial fraction
>[!info] representation
![[Pasted image 20251028171505.png]]
we can divide $p$ by $n$ (by parallelizing it), however we can’t speedup $s$

so we need to re-consider how we think of $T_{parallel}(p)$:
$$
T_{\text{parallel}}(p) = (1-\alpha)T_{\text{serial}}+\alpha \frac{T_{\text{serial}}}{p}
$$
we also need to re-consider the speedup $S(p)$:
$$
S(p) = \frac{T_{\text{serial}}} {(1-\alpha)T_{\text{serial}}+\alpha \frac{T_{\text{serial}}}{p}}
$$

this puts an *upper bound* of the speedup *for any amount of processors*, as
$$
\lim_{ p \to \infty } S(p) = \frac{1}{1-\alpha}
$$
>[!example] example
if $\alpha=0.6$, it means we can expect a speedup of, at most, 2.5
if $\alpha=0.8$, it means we can expect a speedup of, at most, 5
>
>to be able to scale up to 10000, processes, we need to have a $\alpha \geq 0.9999$


>[!info] chart
![[Pasted image 20251028172537.png]]
this chart shows the relationship between $alpha$ and the possible scalability

however, in pratice this could get worse than the hypothesized best case scenario, as with small problem sizes, using more processes creates a lot of overhead 
>[!info] law of diminishing returns in full effect
![[Pasted image 20251103143640.png]]
## gustafson’s law
if we consider *weak scaling*, the parallel fraction increases with the problem size ! also, because the increase in problem size is matched with an increase of processes, the work assigned to each processor remains constant
- the serial time remains constant, because the serial time usually deals with the overall setup and tear-down of the program, not the per-processor work.
- the parallel time increases, as more processes implies more communication and synchronization between them, which increases the overhead. in fact, collective functions get worse in performance the more processes there are inside the communicator
this is also known as *scaled speedup*:
$$
S(n,p) = (1-\alpha) + \alpha p
$$

>[!info] representation
![[Pasted image 20251103143125.png]]

## example: sum between vectors
vector sum:
$$
\begin{align}
x+y&=(x_{0},x_{1},\dots,x_{n-1})+(y_{0},y_{1},\dots,y_{n-1}) \\
&=(x_{0}+y_{0},x_{1}+y_{1},\dots,x_{n-1}+y_{n-1}) \\
&=(z_{0},z_{1},\dots,z_{n-1}) \\
&=\pmb{z}
\end{align}
$$
serial code:
```c
	void vector_sum(double x[], double y[], double z[], int n){
		int i;
		for(i = 0; i < n i++)
			z[i] = x[i] + y[i];
	}
```

first parallel implementation:
```c
void parallel_vector_sum(
		double local_x[], // in
		double local_y[], // in
		double local_z[], // out
		int    local_n    // in
) {
	int local_i;
	for (local_i=0; local_i<local_n; local_i++)
		local_z[local_i] = local_x[local_i] + local_y[local_i];
}

```

## `MPI_Scatter`
`MPI_Scatter` should be used in a function that reads in an entire vector on process 0, and sends the needed component to each of the processes
>[!info] representation
![[Pasted image 20251103144807.png]]

>[!syntax] syntax
>```c
>int MPI_Scatter(
>	void*        send_buf_p, // in
>	int          send_count, // in
>	MPI_Datatype send_type,  // in
>	void*        recv_buf_p, // out
>	int          recv_count, // in
>	MPI_Datatype recv_type,  // in
>	int          src_proc,   // in
>	MPI_Comm     comm        // in
>);
>```
>- `count` is the number of elements to send to each process, not the total number of elements !

>[!info] to send a different number of elements to each rank, we can use the collective function `MPI_Scatterv`
>- this can be needed when the number of elements is not evenly fractionable between the number of processes

`MPI_IN_PLACE` can optimize the `MPI_Scatter` function
(instead of requiring a new buffer for process 0, it uses the fact that process 0 already has the entire buffer)


gather gathers following the ranks as the order


matrices get stored as rows in memory

linearize matrices ! we have to access it in a different way but its fine


how to parallelize matrix multiplication
- we can give matrix and array to every process
- we can divide the rows (and give them to diff processes ) and give (broadcast) the array to each process. then we gather


let’s be annoying. we use the output of the multiplication as the new array to multiply A by


if while writing code i use 2 collective functions in a row (es: reduce + broadcast) , there probably is another collective that does exactly that but in one (es: allreduce)


allgather has no root. everyone sends and receives something


diff machines could implement types with different sizes

CPU lingo
x core e y thread
context switching is very expensive !
having 2 threads in a core means that, with the same ALU, there are resources to save 2 process contexts. that way, context switching is faster